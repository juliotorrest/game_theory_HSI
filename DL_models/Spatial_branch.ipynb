{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf899ec",
   "metadata": {},
   "source": [
    "In order to train the LSTM branch, first we need to import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae6b0075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import datetime\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce47c160",
   "metadata": {},
   "source": [
    "Now, we define some useful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b72f0212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points,factor=0.9):\n",
    "    smoothed_points=[]\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous=smoothed_points[-1]\n",
    "            smoothed_points.append(previous*factor+point*(1-factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "def plot_loss(history, directory, bias=0, cols=2):\n",
    "    # summarize history for accuracy\n",
    "    plt.figure(figsize=(20,7))\n",
    "    keys=list(history.keys())\n",
    "    rows=math.ceil(len(history.keys())/4)\n",
    "    counter=math.ceil(len(history.keys())/2)\n",
    "\n",
    "    for i in range(counter):\n",
    "        plt.subplot(rows,cols,i+1)\n",
    "        plt.plot(smooth_curve(history[keys[i]][bias:]))\n",
    "        plt.plot(smooth_curve(history[keys[i+counter]][bias:]))\n",
    "        plt.ylabel('metric')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend([keys[i],keys[i+counter]], loc='upper right')\n",
    "        \n",
    "    plt.show()\n",
    "    file=os.path.join(directory,'training_metrics.png')\n",
    "    plt.savefig(file)\n",
    "    \n",
    "def preprocess(data_cube, pixels_per_sample=1, average='True', dim=3):\n",
    "    #Data preprocessing\n",
    "    idx=[]\n",
    "    data_prepro=[]\n",
    "    data_ready=np.reshape(data_cube,(-1,150))\n",
    "    for i in range(len(data_ready)):\n",
    "        if data_ready[i].all()==0: idx.append(i)\n",
    "    #delete black pixels\n",
    "    data_ready=np.delete(data_ready,idx,axis=0)\n",
    "    #reshape data\n",
    "    if  average=='Full':#average over ALL pixels\n",
    "        data_prepro=np.mean(data_ready,axis=0,keepdims=True)\n",
    "        data_prepro=np.nan_to_num(data_prepro)\n",
    "        data_prepro=np.reshape(data_prepro,(1,150,1))\n",
    "    else:\n",
    "        for j in range(data_ready.shape[0]//pixels_per_sample):\n",
    "            if average=='False' or pixels_per_sample==1:\n",
    "                data_prepro.append(data_ready[j:j+pixels_per_sample])            \n",
    "            else:\n",
    "                #Calculates average of the pixels if needed\n",
    "                data_aux=np.mean(data_ready[j:j+pixels_per_sample],axis=(0))\n",
    "                data_aux=np.reshape(data_aux,(1,-1))\n",
    "                data_prepro.append(data_aux)\n",
    "                \n",
    "        data_prepro=np.asarray(data_prepro)\n",
    "    \n",
    "        if dim==3: \n",
    "            data_prepro=np.swapaxes(data_prepro,1,2)\n",
    "        elif dim==2:\n",
    "            data_prepro=np.reshape(data_prepro,(-1,150))\n",
    "        else:\n",
    "            print('Wrong data dimensions selected.')\n",
    "    \n",
    "    return data_prepro\n",
    "\n",
    "    \n",
    "#Data generator\n",
    "def hs_generator(h5f, gt='moisture', mode='train',\n",
    "                 pixels_per_sample=1, average='True', dim=3, full_sample=False,\n",
    "                 scaled=False, box_size=10):\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        for key in h5f.keys():\n",
    "            data_temp=h5f[key]\n",
    "            print('\\nLoading group: ', key)\n",
    "            \n",
    "            for key in data_temp.keys():\n",
    "                rand_split=np.random.randint(100)\n",
    "                        \n",
    "                if mode=='train':\n",
    "                    if key[0]!='4' and rand_split<=80: #80% for training\n",
    "                        #print('\\nTraining set. Loading HS image from plot: ', key)\n",
    "                        hs_temp=data_temp[key]\n",
    "                        #Obtains the target value\n",
    "                        for key in hs_temp.attrs.keys():\n",
    "                            if key==gt:\n",
    "                                print('{} => {}'.format(key, hs_temp.attrs[key]))\n",
    "                                target=hs_temp.attrs[key]\n",
    "                                \n",
    "                         #Gets the data in the hdf5 file\n",
    "                        sample_hs=hs_temp[()]\n",
    "                        if full_sample==True:\n",
    "                            sample=hs_temp[()]\n",
    "                            if scaled==True:\n",
    "                                small_sample=[]\n",
    "                                for i in range(int(np.floor(sample.shape[0]/box_size))):\n",
    "                                    for j in range(int(np.floor(sample.shape[1]/box_size))):\n",
    "                                        min_box=sample[i*box_size:(i+1)*box_size,j*box_size:(j+1)*box_size,:]\n",
    "                                        small_sample.append(min_box)\n",
    "                                sample=np.asarray(small_sample)\n",
    "                        else:\n",
    "                            sample=preprocess(sample_hs,pixels_per_sample,average,dim)  \n",
    "                            \n",
    "                        if gt=='treatment':\n",
    "                            if target[:4]=='L233':\n",
    "                                target=0\n",
    "                            else:\n",
    "                                target=1\n",
    "                        \n",
    "                        y=np.asarray([target for i in range(sample.shape[0])])\n",
    "                        \n",
    "                        yield sample,y\n",
    "                                                \n",
    "                elif mode=='val':\n",
    "                    if key[0]!='4' and rand_split>80: #20% for validation\n",
    "                        #print('\\nValidation set. Loading HS image from plot: ', key)\n",
    "                        hs_temp=data_temp[key]\n",
    "                        #Obtains the target value\n",
    "                        for key in hs_temp.attrs.keys():\n",
    "                            if key==gt:\n",
    "                                print('{} => {}'.format(key, hs_temp.attrs[key]))\n",
    "                                target=hs_temp.attrs[key]\n",
    "                                \n",
    "                         #Gets the data in the hdf5 file\n",
    "                        sample_hs=hs_temp[()]\n",
    "                        if full_sample==True:\n",
    "                            sample=hs_temp[()]\n",
    "                            if scaled==True:\n",
    "                                small_sample=[]\n",
    "                                for i in range(int(np.floor(sample.shape[0]/box_size))):\n",
    "                                    for j in range(int(np.floor(sample.shape[1]/box_size))):\n",
    "                                        min_box=sample[i*box_size:(i+1)*box_size,j*box_size:(j+1)*box_size,:]\n",
    "                                        small_sample.append(min_box)\n",
    "                                sample=np.asarray(small_sample)\n",
    "                        else:\n",
    "                            sample=preprocess(sample_hs,pixels_per_sample,average,dim)  \n",
    "                        \n",
    "                        if gt=='treatment':\n",
    "                            if target[:4]=='L233':\n",
    "                                target=0\n",
    "                            else:\n",
    "                                target=1\n",
    "                                \n",
    "                        y=np.asarray([target for i in range(sample.shape[0])])\n",
    "                        \n",
    "                        yield sample,y\n",
    "                                            \n",
    "                else:\n",
    "                    ValueError    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4a0d6",
   "metadata": {},
   "source": [
    "Now, we define some variables for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c1287fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt='moisture'\n",
    "num_epochs=2000\n",
    "box_size=20\n",
    "optimizer='Adadelta'\n",
    "out_layer='block3_pool'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7294d",
   "metadata": {},
   "source": [
    "Then we define some directories and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64934af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Opening dataset...\n",
      "\n",
      "General information about the dataset:\n",
      "creator => Julio Torres-Tello\n",
      "institution => University of Saskatchewan\n",
      "year => 2020\n",
      "crop => canola\n",
      "bands => 400-1000nm\n",
      "scanner => Corning microHSI SHARK\n",
      "\n",
      "Groups contained in dataset:\n",
      "03092019_DAS95\n",
      "09092019_DAS101\n",
      "16082019_DAS77\n",
      "16092019_DAS108\n",
      "20082019_DAS81\n",
      "25092019_DAS117\n",
      "27082019_DAS88\n",
      "30082019_DAS91\n"
     ]
    }
   ],
   "source": [
    "main_dir='/your/directory/'\n",
    "data_dir=os.path.join(main_dir,'data_hs')\n",
    "\n",
    "saving_dir=os.path.join(main_dir,'results_github','canola_hsi')\n",
    "string_dir='mkdir -p '+saving_dir\n",
    "os.system(string_dir)\n",
    "\n",
    "#%%\n",
    "#Load data and labels\n",
    "file_name_h5=os.path.join(data_dir,'NUE_canola_hsi_dataset.hdf5')\n",
    "h5f=h5py.File(file_name_h5,'r')\n",
    "\n",
    "print('\\nOpening dataset...')\n",
    "\n",
    "print('\\nGeneral information about the dataset:')\n",
    "for i in h5f.attrs.keys():\n",
    "      print('{} => {}'.format(i, h5f.attrs[i]))\n",
    "\n",
    "print('\\nGroups contained in dataset:')\n",
    "for key in h5f.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "354d7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data generators\n",
    "train_gen_plot=hs_generator(h5f, gt, mode='train', full_sample=True, scaled=True, box_size=box_size)\n",
    "val_gen_plot=hs_generator(h5f, gt, mode='val', full_sample=True, scaled=True, box_size=box_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d54aa65",
   "metadata": {},
   "source": [
    "Now, we implement the spatial (VGG16-pretrained based) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed05d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model created successfully!\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_spatial (InputLayer)   [(None, 20, 20, 150)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 20, 20, 3)         453       \n",
      "_________________________________________________________________\n",
      "model (Model)                multiple                  1735488   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               123000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 85        \n",
      "=================================================================\n",
      "Total params: 1,869,190\n",
      "Trainable params: 133,702\n",
      "Non-trainable params: 1,735,488\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#VGG module\n",
    "vgg_block=VGG16(weights='imagenet', include_top=False)\n",
    "deep_model=Model(inputs=vgg_block.input,outputs=vgg_block.get_layer(out_layer).output)\n",
    "\n",
    "# Create model\n",
    "\n",
    "input_spatial=Input(shape=(box_size, box_size, 150),name='input_spatial')\n",
    "spatial=Conv2D(3,kernel_size=(1,1),activation='relu',\n",
    "               data_format='channels_last')(input_spatial)\n",
    "spatial=deep_model(spatial)\n",
    "\n",
    "fc=Flatten()(spatial)\n",
    "fc=Dense(120,activation='relu')(fc)\n",
    "fc=Dense(84,activation='relu')(fc)\n",
    "output=Dense(1,activation='linear')(fc)\n",
    "\n",
    "model=Model(input_spatial,output)\n",
    "\n",
    "deep_model.trainable=False\n",
    "model.compile(optimizer=optimizer,loss='mape',\n",
    "              metrics=['mae','mse'])\n",
    "\n",
    "print(\"\\nModel created successfully!\\n\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e5e262",
   "metadata": {},
   "source": [
    "Now, we train the model. In this example, we used only 5 epochs; however, the correct value was already mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b927fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model starts training at: 2021-06-24 20:46:10.717611\n",
      "\n",
      "Model trained.\n",
      "\n",
      "Model finishes training at: 2021-06-24 20:46:10.717820\n",
      "Total duration of model training: 0:00:00.000209\n",
      "\n",
      "Training finished correctly!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "callbacks_list=[ModelCheckpoint(os.path.join(saving_dir,'model_lstm.h5'),save_best_only=True)]\n",
    "\n",
    "#Training the model\n",
    "init_time=datetime.datetime.now()\n",
    "print('Model starts training at:',init_time)   \n",
    "reg=model.fit(train_gen,epochs=5,verbose=False,shuffle=True,\n",
    "                        steps_per_epoch=308,validation_data=val_gen,\n",
    "                        validation_steps=77,callbacks=callbacks_list)\n",
    "end_time=datetime.datetime.now()\n",
    "print('Model finishes training at:',end_time)\n",
    "\n",
    "total_time=end_time-init_time\n",
    "print('Total duration of model training:',total_time)\n",
    "\n",
    "print(\"\\nTraining finished correctly!\\n\")\n",
    "\n",
    "#closing data file\n",
    "h5f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
